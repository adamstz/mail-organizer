# llm

Placeholders and orchestration helpers for running a local LLM (e.g., llama.cpp, llama.cpp-based servers, or containerized models).

This folder contains ideas for running a local model service the backend can call.
